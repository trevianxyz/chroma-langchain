{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Question Answering with local persistence\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database. \n",
    "You can store embeddings and documents, then use them again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.conda/lib/python3.10/site-packages (0.0.226)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: chromadb in ./.conda/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.0.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.conda/lib/python3.10/site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in ./.conda/lib/python3.10/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain) (1.25.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./.conda/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.3 in ./.conda/lib/python3.10/site-packages (from chromadb) (2.0.3)\n",
      "Requirement already satisfied: hnswlib>=0.7 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.6.6)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.8.1)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.conda/lib/python3.10/site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.conda/lib/python3.10/site-packages (from chromadb) (4.7.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./.conda/lib/python3.10/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.conda/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.conda/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.conda/lib/python3.10/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: importlib-metadata in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (6.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2.0.3)\n",
      "Requirement already satisfied: pytz in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\n",
      "Requirement already satisfied: zstandard in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
      "Requirement already satisfied: lz4 in ./.conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./.conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: click>=7.0 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.4)\n",
      "Requirement already satisfied: h11>=0.8 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./.conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.7.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.10/site-packages (from importlib-metadata->clickhouse-connect>=0.5.7->chromadb) (3.15.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "#from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredAPIFileLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import JSONLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace.\n",
    "\n",
    "Next we split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.fireflies.ai/graphql'\n",
    "\n",
    "payload = {\n",
    "    \"query\": \"\"\"\n",
    "        query {\n",
    "            transcripts {\n",
    "                title\n",
    "                sentences {\n",
    "                  text\n",
    "                  raw_text\n",
    "                  speaker_name\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer 1ab2cdbd-53fb-40b5-a491-fd395fd2037f'\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "dict = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "s = pd.Series(data=dict['data']['transcripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     {'title': 'Erin Valdez 5.m4a', 'sentences': [{...\n",
      "1     {'title': 'Erin Valdez 6.m4a', 'sentences': [{...\n",
      "2     {'title': 'Christine Carter, Washington, DC', ...\n",
      "3     {'title': 'Erin Valdez 1.m4a', 'sentences': [{...\n",
      "4     {'title': 'Erin Valdez 2.m4a', 'sentences': [{...\n",
      "5     {'title': 'Erin Valdez 3.m4a', 'sentences': [{...\n",
      "6     {'title': 'Erin Valdez 4.m4a', 'sentences': [{...\n",
      "7     {'title': 'Zach Kavanaugh.m4a', 'sentences': [...\n",
      "8     {'title': 'zach kavanaugh p2.m4a', 'sentences'...\n",
      "9     {'title': 'Zach Kavanaugh 3.m4a', 'sentences':...\n",
      "10    {'title': 'Zach Kavanaugh.m4a', 'sentences': [...\n",
      "11    {'title': 'Jeff Long, San Antonio.m4a', 'sente...\n",
      "12    {'title': 'Luis Chavez, Montague, CA', 'senten...\n",
      "13    {'title': 'Andy Petrie', 'sentences': [{'text'...\n",
      "14    {'title': 'Milo Zanko, Adelaide, AUS', 'senten...\n",
      "15    {'title': 'Peggy Myers, Upper P, Michigan', 's...\n",
      "16    {'title': 'Chris Collins', 'sentences': [{'tex...\n",
      "17    {'title': 'Laura Mackay, Claire Creek', 'sente...\n",
      "18    {'title': 'Kay Bailey Hutchison Convention Cen...\n",
      "19    {'title': 'Alcide Salse, NYFD High School', 's...\n",
      "20    {'title': 'Dallas Librarian', 'sentences': [{'...\n",
      "21    {'title': 'Jeff Long, San Antonio, TX', 'sente...\n",
      "22    {'title': 'Crystal Cap- Chattanooga.m4a', 'sen...\n",
      "23    {'title': 'Tori Aldridge, TX', 'sentences': [{...\n",
      "24    {'title': 'Tori Aldridge, Denton, TX', 'senten...\n",
      "25    {'title': 'Jacob Roberts, Colorado', 'sentence...\n",
      "26    {'title': 'Bernie Contreras,  Bokoima, CA', 's...\n",
      "27    {'title': 'Bridget Myers, Highland Park TX', '...\n",
      "28    {'title': 'Chris Gingri, Buna', 'sentences': [...\n",
      "29    {'title': 'Randall Carter2', 'sentences': [{'t...\n",
      "30    {'title': 'Randall Carter', 'sentences': [{'te...\n",
      "31    {'title': 'Robotics Coach Bel Air, CO', 'sente...\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s)\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv('booy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.read_csv('booy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.to_csv('booys2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index                                               text  \\\n",
      "0         0                 So my goal with Stem education at.   \n",
      "1         1             This point would be to incorporate it.   \n",
      "2         2                              Into the science lab.   \n",
      "3         3  I currently see 3rd, fourth and fifth graders ...   \n",
      "4         4              Come in for hands on science lessons.   \n",
      "...     ...                                                ...   \n",
      "1600     42  Again, I know this is educational educator foc...   \n",
      "1601     43                             I see it all the time.   \n",
      "1602     44  Referee making a bad call and the parents gett...   \n",
      "1603     45  Think that could be handled better from the co...   \n",
      "1604     46                                              Yeah.   \n",
      "\n",
      "                                               raw_text  \\\n",
      "0                    So my goal with Stem education at.   \n",
      "1                This point would be to incorporate it.   \n",
      "2                                 Into the science lab.   \n",
      "3     I currently see 3rd, fourth and fifth graders ...   \n",
      "4                 Come in for hands on science lessons.   \n",
      "...                                                 ...   \n",
      "1600  Again, I know this is educational educator foc...   \n",
      "1601                             I see it all the time.   \n",
      "1602  Referee making a bad call and the parents gett...   \n",
      "1603  Think that could be handled better from the co...   \n",
      "1604                                              Yeah.   \n",
      "\n",
      "                    speaker_name  \n",
      "0                    Luis Chavez  \n",
      "1                    Luis Chavez  \n",
      "2                    Luis Chavez  \n",
      "3                    Luis Chavez  \n",
      "4                    Luis Chavez  \n",
      "...                          ...  \n",
      "1600  robotics coach Bel Air, CO  \n",
      "1601  robotics coach Bel Air, CO  \n",
      "1602  robotics coach Bel Air, CO  \n",
      "1603  robotics coach Bel Air, CO  \n",
      "1604  robotics coach Bel Air, CO  \n",
      "\n",
      "[1174 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the CSV data to a DataFrame\n",
    "q4 = pd.read_csv('quotes.csv')\n",
    "\n",
    "# Drop the unwanted columns\n",
    "q5 = q4.drop(['one', 'two', 'start_time', 'end_time', 'speaker_id'], axis=1)\n",
    "\n",
    "q5 = q5[q5['speaker_name'] != 'JM']\n",
    "\n",
    "q5.to_csv('updated_quotes.csv', index=False)\n",
    "# Print the updated DataFrame\n",
    "print(q5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  AI meeting summary:  \\\n",
      "0   [A robotics coach with a background in IT and ...   \n",
      "1   [Alcide Salse, representing the FDNY High Scho...   \n",
      "2   [Jeff Long, a teacher at Stevenson Middle Scho...   \n",
      "3   [Luis Chavez, a science teacher from Montague ...   \n",
      "4   [Crystal Cap from Chattanooga Girls Leadership...   \n",
      "5   [Jacob Roberts is an assistant program manager...   \n",
      "6   [Mill Zankov, a teacher in Adelaide, Australia...   \n",
      "7   [Chris Gingri teaches K-5 engineering lab at B...   \n",
      "8   [Zach Kavanaugh teaches at Duchin, The Academy...   \n",
      "9   [Chris Collins has been teaching robotics for ...   \n",
      "10  [The speaker is impressed with the educational...   \n",
      "11  [Laura Mackay, the Coordinator of Innovative P...   \n",
      "12  [Andy Petrie discusses efforts to involve more...   \n",
      "13  [Peggy Myers, a middle school teacher from Mic...   \n",
      "14  [Bridget Myers, a teacher at Highland Park Mid...   \n",
      "15  [A STEM educator discusses their goals of brin...   \n",
      "16  [Bernie Contreras, a fifth grade teacher at Mo...   \n",
      "17  [Randall Carter, the executive director for Me...   \n",
      "18  [Tori Aldridge, a technology teacher from Dent...   \n",
      "\n",
      "                                             Outline:  \\\n",
      "0   {'Chapter 1: Introduction': ['Timestamp: 0:00-...   \n",
      "1   [I'm sorry, but it seems that there are no tim...   \n",
      "2   [Outline:, I. Introduction (00:00), Introducti...   \n",
      "3   [Sure, here's an outline with timestamps for t...   \n",
      "4   {'Chapter 1: Introduction': ['Timestamp: 0:00-...   \n",
      "5   [I'm sorry, but I don't think it's possible to...   \n",
      "6   [I have reviewed the transcript and identified...   \n",
      "7   [I'm sorry, I cannot create an outline with ch...   \n",
      "8   {'Chapter 1: Introduction to Zach Kavanaugh an...   \n",
      "9   {'Chapter 1: Introduction': ['Timestamp: 0:00-...   \n",
      "10  [I'm sorry, but I don't see any timestamps in ...   \n",
      "11  {'Chapter 1: Introduction': ['0:00-0:09', 'Int...   \n",
      "12  {'Chapter 1: Introduction to STEM Implementati...   \n",
      "13  {'Chapter 1: Introduction': ['Timestamp: 0:00 ...   \n",
      "14  {'Chapter 1: Introduction': ['Timestamp: 0:00-...   \n",
      "15  [I'm sorry, but I cannot provide timestamps as...   \n",
      "16  {'Chapter 1: Introduction to Vex Robotics at M...   \n",
      "17  [I'm sorry, but the transcript does not contai...   \n",
      "18  {'Chapter 1: Introduction': ['Tori Aldridge in...   \n",
      "\n",
      "                                               Notes:  \\\n",
      "0   [The speaker is a robotics coach for the Bel A...   \n",
      "1   [Alcide Salse is representing the New York Cit...   \n",
      "2   [Jeff Long teaches at Stevenson Middle School ...   \n",
      "3   [Luis Chavez, a science teacher at Montague Ch...   \n",
      "4   [Crystal Cap is a middle school computer scien...   \n",
      "5   [Jacob Roberts is an assistant program manager...   \n",
      "6   [Mill Zankov is a teacher in Adelaide, Austral...   \n",
      "7   [Chris Gingri teaches K-5 engineering lab at B...   \n",
      "8   [Zach Kavanaugh teaches at Duchin, The Academy...   \n",
      "9   [Chris Collins has been teaching Vex IQ for si...   \n",
      "10  [Sure! Here are some shorthand bullet-point no...   \n",
      "11  [Laura Mackay is the coordinator of Innovative...   \n",
      "12  [The school wants more year levels involved in...   \n",
      "13  [Peggy Myers is a middle school teacher at Han...   \n",
      "14  [Bridget Myers teaches at Highland Park Middle...   \n",
      "15  [The speaker's goal with STEM education is to ...   \n",
      "16  [The speaker is a fifth grade teacher at Monta...   \n",
      "17  [Name: Randall Carter, Job Title: Executive Di...   \n",
      "18  [Tori Aldridge is a technology teacher from De...   \n",
      "\n",
      "                                        Action items:  \\\n",
      "0   [Follow-ups:, What is the Vex robotics competi...   \n",
      "1   [Follow-ups:, None explicitly stated., Action ...   \n",
      "2   [Follow-ups:, 1. What are your goals with Stem...   \n",
      "3   [Follow-ups:, 1. What is Luis Chavez's experie...   \n",
      "4   [1. Follow-up: Request for goals related to ST...   \n",
      "5   [Based on the transcript, there are several fo...   \n",
      "6   [There are a few potential follow-ups and acti...   \n",
      "7   [1. The speaker mentions integrating robotics ...   \n",
      "8   [Follow-ups:, What subjects do you teach at Du...   \n",
      "9   [Follow-ups:, 1. What are Chris Collins' goals...   \n",
      "10  [I'm sorry, but I cannot infer any specific fo...   \n",
      "11  [Follow-ups:, None explicitly requested., Acti...   \n",
      "12  [Based on the transcript, here are the follow-...   \n",
      "13  [Follow-ups:, 1. What are your goals with STEM...   \n",
      "14  [1. Follow-up: None mentioned explicitly., 2. ...   \n",
      "15  [From the transcript, some follow-ups and acti...   \n",
      "16  [From the transcript, the following follow-ups...   \n",
      "17  [From the transcript, I can infer that there w...   \n",
      "18  [From the transcript, we can infer the followi...   \n",
      "\n",
      "                                        Action Items:  \n",
      "0                                                 NaN  \n",
      "1                                                 NaN  \n",
      "2                                                 NaN  \n",
      "3                                                 NaN  \n",
      "4                                                 NaN  \n",
      "5                                                 NaN  \n",
      "6                                                 NaN  \n",
      "7                                                 NaN  \n",
      "8                                                 NaN  \n",
      "9                                                 NaN  \n",
      "10                                                NaN  \n",
      "11                                                NaN  \n",
      "12                                                NaN  \n",
      "13  [1. The respondent expressed interest in hosti...  \n",
      "14                                                NaN  \n",
      "15                                                NaN  \n",
      "16                                                NaN  \n",
      "17                                                NaN  \n",
      "18                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store the JSON data\n",
    "data_list = []\n",
    "\n",
    "# Open the JSONL file for reading\n",
    "with open('output.jsonl', 'r') as file:\n",
    "    # Read each line of the file\n",
    "    for line in file:\n",
    "        # Parse the JSON data from each line\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Append the JSON data to the list\n",
    "        data_list.append(data)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Change the index from rows to columns\n",
    "#df = df.transpose()\n",
    "\n",
    "df = df.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "df.to_csv('new_sm.csv', index=False)\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Open the CSV file and read the data into a DataFrame\n",
    "this_data = pd.read_csv('new_sm.csv')\n",
    "\n",
    "# Define the headers\n",
    "headers = ['AI meeting summary', 'Outline', 'Notes', 'Action items', 'Action Items']\n",
    "\n",
    "# Create a list to store the processed data\n",
    "processed_data = []\n",
    "\n",
    "# Iterate through the data and remove special characters\n",
    "for column in this_data.columns:\n",
    "    processed_column = this_data[column].apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", str(x)))\n",
    "    processed_data.append(processed_column)\n",
    "\n",
    "# Write the processed data to a new markdown file\n",
    "with open(\"processed_data.md\", \"w\") as file:\n",
    "    for i in range(len(headers)):\n",
    "        file.write(\"# \" + headers[i] + \"\\n\")\n",
    "        file.write(processed_data[i].to_string(index=False) + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown file 'processed_pages.md' saved.\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame 'this_data'\n",
    "\n",
    "with open('new_sm.csv', 'r') as file:\n",
    "\n",
    "    this_data = pd.DataFrame(file)\n",
    "\n",
    "# Create a list to store the pages\n",
    "pages = []\n",
    "\n",
    "# Iterate through the columns\n",
    "for column in this_data.columns:\n",
    "    # Get the text from the column\n",
    "    text = this_data[column]\n",
    "    \n",
    "\n",
    "    # Add the processed text to the pages list\n",
    "    processed_pages.append(processed_text)\n",
    "\n",
    "# Write the processed pages to a new markdown file\n",
    "markdown_filename = 'processed_pages.md'\n",
    "with open(markdown_filename, 'w') as file:\n",
    "    for page_number, page in enumerate(processed_pages, 20):\n",
    "        file.write(f\"Page {page_number}:\\n\\n\")\n",
    "        file.write(page)\n",
    "        file.write(\"\\n\\n----------------------\\n\")\n",
    "\n",
    "print(f\"Markdown file '{markdown_filename}' saved.\")\n",
    "\n",
    "#     # Add the text to the pages list\n",
    "#     pages.append(text)\n",
    "\n",
    "# # Print the pages\n",
    "# for page_number, page in enumerate(pages, 1):\n",
    "#     print(f\"Page {page_number}:\\n\")\n",
    "#     print(page)\n",
    "#     print(\"\\n----------------------\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PeristedChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database. The `persist_directory` argument tells ChromaDB where to store the database when it's persisted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./.conda/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "%pip install tiktoken\n",
    "\n",
    "loader = CSVLoader(file_path='./new_sm.csv')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the Database\n",
    "In a notebook, we should call `persist()` to ensure the embeddings are written to disk.\n",
    "This isn't necessary in a script - the database will be automatically persisted when the client object is destroyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Database from disk, and create the chain\n",
    "Be sure to pass the same `persist_directory` and `embedding_function` as you did when you instantiated the database. Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for RetrievalQA\nretriever\n  field required (type=value_error.missing)\nvectorstore\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[539], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Now we can load the persisted database from disk, and use it as normal.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma(persist_directory\u001b[39m=\u001b[39mpersist_directory, embedding_function\u001b[39m=\u001b[39membedding)\n\u001b[0;32m----> 3\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39;49mfrom_chain_type(llm\u001b[39m=\u001b[39;49mOpenAI(), chain_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstuff\u001b[39;49m\u001b[39m\"\u001b[39;49m, vectorstore\u001b[39m=\u001b[39;49mvectordb)\n",
      "File \u001b[0;32m~/Documents/repos/vexllm/chroma-langchain/.conda/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:95\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m _chain_type_kwargs \u001b[39m=\u001b[39m chain_type_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m     92\u001b[0m combine_documents_chain \u001b[39m=\u001b[39m load_qa_chain(\n\u001b[1;32m     93\u001b[0m     llm, chain_type\u001b[39m=\u001b[39mchain_type, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_chain_type_kwargs\n\u001b[1;32m     94\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(combine_documents_chain\u001b[39m=\u001b[39;49mcombine_documents_chain, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/repos/vexllm/chroma-langchain/.conda/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/repos/vexllm/chroma-langchain/.conda/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for RetrievalQA\nretriever\n  field required (type=value_error.missing)\nvectorstore\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal.\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the top themes in these summaries?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm not sure what you're asking about.\""
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"tell me more about this?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done with the database, you can delete it from disk. You can delete the specific collection you're working with (if you have several), or delete the entire database by nuking the persistence directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting DB to disk, putting it in the save folder db\n"
     ]
    }
   ],
   "source": [
    "# To cleanup, you can delete the collection\n",
    "vectordb.delete_collection()\n",
    "vectordb.persist()\n",
    "\n",
    "# Or just nuke the persist directory\n",
    "!rm -rf db/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
